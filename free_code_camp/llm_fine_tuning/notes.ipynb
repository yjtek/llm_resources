{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Changing floating point numbers from 32bits to lower precision, thereby reducing memory use (full precision to half precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quantization involves 2 parameters: a `scale` and a `zero point`\n",
    "    - Symmetric Quantization\n",
    "        - Imagine I have am array of 32-bit floating point values in range [0, 1000]. \n",
    "        - What happens if I want to convert these values into unsigned 8bit integers? (i.e. in range [0, 255])\n",
    "        - Note: Understand how the floating point values are stored \n",
    "            - A 32-bit float are split into\n",
    "                - 1 bit for sign\n",
    "                - 8 bits for exponent\n",
    "                - 23 bits for mantissa (fractional value)\n",
    "            - A 16-bit float are split into\n",
    "                - 1 bit for sign\n",
    "                - 5 bits for exponent\n",
    "                - 10 bits for mantissa (fractional value)\n",
    "\n",
    "        - We scale the 32-bit value using min-max scaling\n",
    "            - Compute the scaling factor: $\\frac{1000-0}{255-0} = 3.92$\n",
    "            - Then, for any 32-bit value $x$, convert to uint-8 by taking $x / 3.92$, and round it\n",
    "\n",
    "        - Batch Norm is a type of symmetric quantization\n",
    "\n",
    "    - Asymmetric Quantization\n",
    "        - Imagine now I have am array of 32-bit floating point values in range [-20, 1000]\n",
    "        - What happens if I want to convert these values into unsigned 8bit integers? (i.e. in range [0, 255])\n",
    "        - We scale the 32-bit value using the same min-max scaling\n",
    "            - Compute the scaling factor: $\\frac{1000 - (-20)}{255-0} = 4$\n",
    "            - Then, for any 32-bit value $x$, convert to uint-8 by taking $x / 4$, and round it\n",
    "        \n",
    "        - But now we have a problem. imagine my initial 32bit float is -20. After conversion, it becomes -5, still below the range of the desired value!\n",
    "\n",
    "        - Hence, in such cases, a new parameter `zero point` is introduced and added post scaling, so the minimum value maps to 0. In this case, `zero point = 5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Training Quantization (PTQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose we have a pretrained model that's extremely large. W\n",
    "- We want to use it, but the weights don't fit in memory. What's the next step?\n",
    "- First, calibrate the model\n",
    "    - Squeeze the range of the values into a smaller range (calibration)\n",
    "    - Convert weights into lower precision (quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Aware Training (QAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In post training quantization, there is inevitably data loss, and so this impacts accuracy\n",
    "- QAT avoids this by adding a fine-tuning step after quantization. So take new training data and do a new round of `fit`\n",
    "- This creates better outcomes for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's say we have a pre-trained base model (say GPT4)\n",
    "    - We can train all parameters based on the data we have, which we'll call \"Full Parameter Fine Tuning\"\n",
    "    - OR we can fine tune the model for specific domains (e.g. sales chat bot, etc). We can call this \"domain specific fine tuning\"\n",
    "    - OR we can fine tune the model for specific tasks (document retrieval bot, q&a bot etc). We can call this \"task specific fine tuning\"\n",
    "\n",
    "- Full Parameter Fine Tuning\n",
    "    - Suppose we have a base model with 175B weights\n",
    "    - In this case, to perform full parameter fine tuning, every run needs you to compute 175B gradients. This is obviously horrible, because you probably don't have the hardware needed to do this\n",
    "    - This is where `LORA` and `QLORA` come in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Low Rank Adaptation of Large Language Models\n",
    "    - Suppose I have pretrained weights denoted by set $S$ \n",
    "    - Initialize a new set of weights $L$ with the same size as $S$. These are the LORA weights\n",
    "    - $L$ will be updated, and the final weights applied for inference will be $L + S$\n",
    "\n",
    "- Won't the same resource constraint with $S$ also be present in $L$ if the shapes are the same?\n",
    "    - We can use matrix decomposition to simplify $L$;\n",
    "    - So instead of, for example, one single $N \\times N$ matrix with $N^2$ elements, we can have $1 \\times N$ and $N \\times 1$ with $2N$ elements\n",
    "\n",
    "- Therefore, the LORA equation is simply $W_0 + \\triangle W = W_0 + B \\cdot A$, where $B \\cdot A$ is simply the matrix decomposition of the LORA weight matrix\n",
    "\n",
    "- Reduction in weights: Suppose we want to decompose an $m \\times n$ matrix with 7B parameters into 2 matrices. For simplicity, assume $m=n$\n",
    "    - For rank 1, the 2 matrices have dimensions $n \\times 1$ and $1 \\times n$. Therefore, $n = \\sqrt{7B} \\approx 83,000$, and total parameters get reduced to $83,000 * 2 \\approx 167,000$\n",
    "    - For rank 8, the 2 matrices have dimensions $n \\times 8$ and $8 \\times n$. Therefore, $n = 83,000$, and total parameters get reduced to $83,000 * 2 * 8 \\approx 1M$\n",
    "\n",
    "| Rank | 7B | 13B | 17B | 180B |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1 | 83K * 2 | 114K * 2 | 130K * 2 | 424K * 2 |\n",
    "| 8 | 83K * 2 | 114K * 2 | 130K * 2 | 424K * 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424264.0687119285"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "180_000_000_000 ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quantized Low Rank Adaptation of Large Language Models\n",
    "- Instead of leaving the LORA weights as they are, reduce precision of LORA weights. That's all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Bit LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of 32bit precision float, or quantization to 8bit, we reduce the weights to ternary operators {-1,0,1}\n",
    "- Why is this great? \n",
    "    - If $W \\in [-1,0,1]$, then $W \\cdot x$ reduces to $\\sum_{w_i = 1} x - \\sum_{w_i = -1} x$\n",
    "    - No more matrix multiplication! Only summation is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open source models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Llama\n",
    "- Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-fine-tuning-In5cv3eh-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
