{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are LLMs trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take training text\n",
    "- Feed training text to tokenizer, with each token assigned a unique index, and append a stop token\n",
    "- Take a window with some length `w` from your training text, and set the target to the `w+1`-th token\n",
    "- Train model on such observations\n",
    "- Model predicts next token iteratively (i.e. given input of 5 words, predict token 6. Then concat the 5 tokens with the 6th and predict again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are LLMs fine tuned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LORA/QLORA\n",
    "- See section on `llm_fine_tuning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to align LLM outputs with human values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RLHF\n",
    "    1. Collect supervised data and train a supervised policy\n",
    "        - sample prompt from prompt dataset\n",
    "        - human labeller will fill in the \"ideal\" output\n",
    "        - fine tune on GPT3 with supervised learning (Supervised Fine Tuning [SFT])\n",
    "    2. Collect comparison data and train \n",
    "        - For a given prompt, generate multiple model outputs\n",
    "        - Human ranks the output from best to worst\n",
    "        - Data used to train the reward model\n",
    "    3. Optimise policy again the reward model using reinforcement learning\n",
    "        - New prompt is sampled from the dataset\n",
    "        - GPT generates an output\n",
    "        - Reward model will score this output\n",
    "        - Update the policy using Proximate Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
