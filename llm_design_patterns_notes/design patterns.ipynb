{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General LLM App Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "chain.invoke({'input1': input1})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt\n",
    "    - `template = ... {schema} ... {question} ... {query} ...`\n",
    "        - Define a prompt with changeable inputs denoted by `{}`\n",
    "    - ```python\n",
    "        prompt_response = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Given an input question and SQL response, convert it to a natural language answer. No pre-amble.\",\n",
    "                ),\n",
    "                (\"human\", template),\n",
    "            ]\n",
    "        )\n",
    "        ```\n",
    "        - Define a `ChatPromptTemplate` that takes in the defined template string\n",
    "        - Either you can assign the variables in `{}` through the chain (e.g. `RunnablePassthrough.assign(schema=get_schema)`), or you can invoke it in the eventual function call\n",
    "    - If you need to have a temporarily nullable object in your chain, use `MessagesPlaceholder(variable_name=\"history\")`. In this case, an empty list is created for the `history` object in case it does not yet exist yet\n",
    "\n",
    "    - For long running contexts, init a `ConversationBufferMemory()` to store messages \n",
    "        - If you need memory to be shared, there are many types of memory. Another one to consider is `ReadOnlySharedMemory`\n",
    "\n",
    "    - Choose most appropriate chain\n",
    "        - For exmample: `SQLDatabaseChain` for natural language to SQL \n",
    "        - For example: `SmartLLMChain` for hard questions\n",
    "\n",
    "    - Customise chain from `Chain` base class\n",
    "        - See https://github.com/langchain-ai/langchain/blob/master/cookbook/sales_agent_with_context.ipynb\n",
    "\n",
    "    - Customise prompt using base class `StringPromptTemplate`\n",
    "        - See https://github.com/langchain-ai/langchain/blob/master/cookbook/sales_agent_with_context.ipynb\n",
    "\n",
    "    - Customise parser using base class `AgentOutputParser`\n",
    "        - See https://github.com/langchain-ai/langchain/blob/master/cookbook/sales_agent_with_context.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Index to Database\n",
    "    - To store texts, it is often necessary to chunk them first\n",
    "        ```python\n",
    "            text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "                chunk_size=4000, chunk_overlap=0\n",
    "            )\n",
    "            texts_4k_token = text_splitter.split_text(joined_texts)\n",
    "        ``` \n",
    "    \n",
    "    - Store chunked texts\n",
    "        ```python\n",
    "            # Create chroma\n",
    "            vectorstore = Chroma(\n",
    "                collection_name=\"mm_rag_clip_photos\", embedding_function=OpenCLIPEmbeddings()\n",
    "            )\n",
    "            # Add images\n",
    "            vectorstore.add_images(uris=image_uris)\n",
    "\n",
    "            # Add documents\n",
    "            vectorstore.add_texts(texts=texts)\n",
    "\n",
    "            # Make retriever\n",
    "            retriever = vectorstore.as_retriever()\n",
    "\n",
    "            # Get docs\n",
    "            docs = retriever.invoke(\"Woman with children\", k=10)\n",
    "            ```\n",
    "\n",
    "    - You can index via the usual text embedding, but there are also multimodal embeddings for non-text data (e.g. langchain_experimental.open_clip.OpenCLIPEmbeddings)\n",
    "\n",
    "    - For cases where you have nontext data, you can try using the `unstructured` package (e.g. `from unstructured.partition.pdf import partition_pdf`)\n",
    "\n",
    "    - In cases where there are many documents, it can benefit from recursive clustering and indexing (i.e. you keep clustering and getting GPT to summarise cluster, then add the summary doc to the DB)\n",
    "        - This is known as RAPTOR\n",
    "        - See https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb\n",
    "\n",
    "    - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retrieve information \n",
    "    - Make retriever/loader\n",
    "        - https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/\n",
    "\n",
    "    - Retrieve from regular SQL database\n",
    "        - `def get_schema(): db.get_table_info()` \n",
    "            - To get database info \n",
    "        - `RunnablePassthrough.assign(schema=get_schema) | prompt` \n",
    "            - To pass schema data into prompt\n",
    "\n",
    "        - When retrieving from database, may also be useful to set AttributeInfo for fields `from langchain.chains.query_constructor.base import AttributeInfo`\n",
    "            - By doing this, you let the LLM decide what to call\n",
    "            - This is also known as \"self-query\"\n",
    "\n",
    "\n",
    "    - Retrieve from regular docsearch database\n",
    "        - ```python\n",
    "            retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "            llm = ChatOpenAI()\n",
    "            retriever = ...\n",
    "            combine_docs_chain = create_stuff_documents_chain(\n",
    "                llm, retrieval_qa_chat_prompt\n",
    "            )\n",
    "            retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "            ```\n",
    "\n",
    "    - Retrieval can be multivector (i.e. multiple types of documents)\n",
    "        - ```python\n",
    "            retriever = MultiVectorRetriever(\n",
    "                vectorstore=vectorstore,\n",
    "                docstore=store,\n",
    "                id_key=id_key,\n",
    "            )\n",
    "\n",
    "            def create_multi_vector_retriever(...)\n",
    "            ```\n",
    "        - See https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb\n",
    "\n",
    "    - You can customise retrievers if you want, inherit from `BaseRetriever`\n",
    "        - See https://github.com/langchain-ai/langchain/blob/master/cookbook/forward_looking_retrieval_augmented_generation.ipynb\n",
    "\n",
    "    - HyDE ( Hypothetical Document Embeddings)\n",
    "        - Instead of using only a single document, generate multiple \"synthetic\" documents and generate an average embedding for better context\n",
    "        - See https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb\n",
    "\n",
    "    - Embedding quantization\n",
    "        - You can quantize the embeddings to ensure smaller memory footprint\n",
    "        - https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_with_quantized_embeddings.ipynb\n",
    "\n",
    "    - Use models to generate embeddings `from langchain_community.embeddings import QuantizedBiEncoderEmbeddings`\n",
    "        - See https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_with_quantized_embeddings.ipynb\n",
    "\n",
    "    - Incorporating a `rewriter` chain into a regular chain can be useful. Think of it as query enhancement as part of your flow\n",
    "        - ```python\n",
    "            rewrite_retrieve_read_chain = (\n",
    "                {\n",
    "                    \"context\": {\"x\": RunnablePassthrough()} | rewriter | retriever,\n",
    "                    \"question\": RunnablePassthrough(),\n",
    "                }\n",
    "                | prompt\n",
    "                | model\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "            ```\n",
    "        - `https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb`\n",
    "\n",
    "    - Sometimes, when pulling multiple documents where the answer is scattered over all documents, the LLM ill get confused\n",
    "        - It is helpful to re-phrase the question into multiple steps, (i.e. take a step back) before answering the qn\n",
    "        - Use \"step back prompting\" --> https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add tools for agent to use\n",
    "    - ```python\n",
    "        tools = [\n",
    "            Tool(\n",
    "                name=\"State of Union QA System\",\n",
    "                func=state_of_union.run,\n",
    "                description=\"useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question.\",\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"Ruff QA System\",\n",
    "                func=ruff.run,\n",
    "                description=\"useful for when you need to answer questions about ruff (a python linter). Input should be a fully formed question.\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        agent = initialize_agent(\n",
    "            tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    "        )\n",
    "        ```\n",
    "      - https://github.com/langchain-ai/langchain/blob/master/cookbook/agent_vectorstore.ipynb\n",
    "\n",
    "    - Tool for search web\n",
    "        - search = SerpAPIWrapper()\n",
    "\n",
    "    - Run another chain as a tool\n",
    "        - ```python\n",
    "            todo_prompt = PromptTemplate.from_template(\"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}\")\n",
    "            todo_chain = LLMChain(llm=OpenAI(temperature=0), prompt=todo_prompt)\n",
    "            ```\n",
    "\n",
    "    - You can define anything as a tool using the `func` parameter, as long as the input is a callable function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add info\n",
    "    - `... | llm.bind(stop=[\"\\nSQLResult:\"]) | `\n",
    "        - Bind results to llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic Chain / Add decision nodes\n",
    "    - See all chain types\n",
    "        - https://python.langchain.com/v0.1/docs/modules/chains/\n",
    "    - `PALChain` for generating pseudocode as part of reasoning\n",
    "        - Does not give you a REPL to run the code, only generates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Models\n",
    "    - Download from `huggingface_hub` if needed\n",
    "        ```python\n",
    "            from huggingface_hub import hf_hub_download\n",
    "            hf_hub_download(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_name, local_dir=\"state\")\n",
    "        ```\n",
    "\n",
    "        - see https://github.com/langchain-ai/langchain/blob/master/cookbook/apache_kafka_message_handling.ipynb\n",
    "\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output parsers\n",
    "    - ` ... | StrOutputParser()`\n",
    "        - Force output to specific format\n",
    "\n",
    "    - Force output to pydantic format\n",
    "        ```python\n",
    "            class code(BaseModel):\n",
    "                \"\"\"Code output\"\"\"\n",
    "\n",
    "                prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "                imports: str = Field(description=\"Code block import statements\")\n",
    "                code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "            # LLM\n",
    "            llm = ChatAnthropic(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                default_headers={\"anthropic-beta\": \"tools-2024-04-04\"},\n",
    "            )\n",
    "\n",
    "            # Structured output, including raw will capture raw output and parser errors\n",
    "            structured_llm = llm.with_structured_output(code, include_raw=True)\n",
    "            code_output = structured_llm.invoke(\n",
    "                \"Write a python program that prints the string 'hello world' and tell me how it works in a sentence\"\n",
    "            )\n",
    "            ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Invocation\n",
    "    - You can invoke multiple prompts at once using `.batch()`\n",
    "        - `chain.batch(texts, {\"max_concurrency\": 5})`\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation\n",
    "    \n",
    "    - See https://github.com/langchain-ai/langchain/blob/master/cookbook/advanced_rag_eval.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extraction\n",
    "    - OpenAI supports extraction casted to pydantic types\n",
    "        - You can coerce to pydantic type using `PydanticToolsParser` if you don't want to use the prebuilt chain\n",
    "            - `chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)`\n",
    "        - ```python\n",
    "            # Make sure to use a recent model that supports tools\n",
    "            model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "            # Pydantic is an easy way to define a schema\n",
    "            class Person(BaseModel):\n",
    "                \"\"\"Information about people to extract.\"\"\"\n",
    "\n",
    "                name: str\n",
    "                age: Optional[int] = None\n",
    "\n",
    "            chain = create_extraction_chain_pydantic(Person, model)\n",
    "            ```\n",
    "        - See https://github.com/langchain-ai/langchain/blob/master/cookbook/extraction_openai_tools.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RLHF\n",
    "    - `HumanApprovalCallbackhandler`\n",
    "        - https://github.com/langchain-ai/langchain/blob/master/cookbook/human_approval.ipynb\n",
    "    - Another way you can handle feedback to an LLM is through meta prompting. That is, given a prompt and response, incorporate an `input()` for human to give feedback. Then use the LLM to generate a \"critique\" to pass on to itself based on `input()`. This critique is then used in the subsequent loops for the LLM to generate better answers\n",
    "        - https://github.com/langchain-ai/langchain/blob/master/cookbook/meta_prompt.ipynb\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langgraph\n",
    "    - Define edges, nodes, tools\n",
    "    - Using `@tools`\n",
    "        - https://github.com/langchain-ai/langchain/blob/master/cookbook/tool_call_messages.ipynb\n",
    "\n",
    "    - You can use tools from an agent or as a usual chat model\n",
    "        - Tools can be called via function \n",
    "            ```python\n",
    "                search_tool = TavilySearchResults(max_results=1, args_schema=SearchTool)\n",
    "                tools = [search_tool]\n",
    "\n",
    "                from langgraph.prebuilt import ToolExecutor\n",
    "                tool_executor = ToolExecutor(tools)\n",
    "\n",
    "                def call_tool(state):\n",
    "                    messages = state[\"messages\"]\n",
    "                    last_message = messages[-1]\n",
    "                    tool_call = last_message.tool_calls[0]\n",
    "                    action = ToolInvocation(\n",
    "                        tool=tool_call[\"name\"],\n",
    "                        tool_input=tool_call[\"args\"],\n",
    "                    )\n",
    "                    response = tool_executor.invoke(action)                    \n",
    "                    function_message = ToolMessage(\n",
    "                        content=str(response), name=action.tool, tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                    return {\"messages\": [function_message]}\n",
    "                    \n",
    "                workflow.add_node(\"action\", call_tool)\n",
    "            ```   \n",
    "        - Tools can be called directly using `tool_node = ToolNode(tools)`\n",
    "            ```python\n",
    "                tool_node = ToolNode(tools)\n",
    "                workflow.add_node(\"action\", tool_node)\n",
    "            ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
